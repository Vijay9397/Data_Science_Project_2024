{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Project 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will procced according to the KDD Process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code first load all the .log files in the..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Folder path containing all .log files\n",
    "folder_path = r\"C:\\Users\\vijay\\Desktop\\SimplyTagUpdated\\All Data\"  # Replace with your folder path\n",
    "output_csv_path = r\"C:\\Users\\vijay\\Desktop\\SimplyTagUpdated\\Analysis\\merged_logs.csv\"\n",
    "\n",
    "# Initialize a list to store JSON data from all files\n",
    "log_data = []\n",
    "\n",
    "# Process each .log file in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith(\".log\"):  # Check if the file is a .log file\n",
    "        log_file_path = os.path.join(folder_path, file_name)\n",
    "        print(f\"Processing file: {log_file_path}\")\n",
    "        \n",
    "        # Read the log file line by line with 'utf-8' encoding and error handling\n",
    "        with open(log_file_path, \"r\", encoding=\"utf-8\", errors=\"replace\") as file:\n",
    "            for line in file:\n",
    "                # Find the JSON part of each line\n",
    "                json_start = line.find('{')\n",
    "                if json_start != -1:\n",
    "                    json_data = line[json_start:].strip()\n",
    "                    try:\n",
    "                        # Parse the JSON and append it to the list\n",
    "                        log_data.append(json.loads(json_data))\n",
    "                    except json.JSONDecodeError:\n",
    "                        print(f\"Skipping invalid JSON in file {file_name}: {json_data}\")\n",
    "\n",
    "# Convert the combined list of JSON objects to a DataFrame\n",
    "if log_data:\n",
    "    df = pd.DataFrame(log_data)\n",
    "\n",
    "    # Save the DataFrame to a single CSV file\n",
    "    df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"All log data successfully merged and saved to {output_csv_path}\")\n",
    "else:\n",
    "    print(\"No valid log entries found in the provided files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:\\Users\\vijay\\Desktop\\SimplyTagUpdated\\Analysis\\merged_logs.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)\n",
    "df.describe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Data Selection:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following line..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/vatsal/Desktop/SimplyTag/merged_logs.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Load the CSV file\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/vatsal/Desktop/SimplyTag/merged_logs.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, low_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Check a sample entry to understand the structure\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample Entry from \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mState\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mState\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/vatsal/Desktop/SimplyTag/merged_logs.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"/Users/vatsal/Desktop/SimplyTag/merged_logs.csv\", low_memory=False)\n",
    "\n",
    "# Check a sample entry to understand the structure\n",
    "print(\"Sample Entry from 'State':\\n\", df['State'].iloc[0])\n",
    "\n",
    "# Function to safely parse JSON\n",
    "def parse_json_safe(json_str):\n",
    "    try:\n",
    "        # Replace single quotes with double quotes for valid JSON\n",
    "        return json.loads(json_str.replace(\"'\", '\"'))\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        return None\n",
    "\n",
    "# Parse 'Scope' and 'State' columns\n",
    "df['Scope'] = df['Scope'].apply(lambda x: parse_json_safe(x) if isinstance(x, str) else None)\n",
    "df['State'] = df['State'].apply(lambda x: parse_json_safe(x) if isinstance(x, str) else None)\n",
    "\n",
    "# Initialize a new DataFrame for selected data\n",
    "df_selected0 = pd.DataFrame()\n",
    "\n",
    "# Extract specific fields after parsing JSON\n",
    "df_selected0['Timestamp'] = df['Timestamp']\n",
    "df_selected0['Trace-id'] = df['Scope'].apply(lambda x: x.get('TraceId') if isinstance(x, dict) else None)\n",
    "df_selected0['HTTP Status Code'] = df['State'].apply(lambda x: x.get('StatusCode') if isinstance(x, dict) else None)\n",
    "df_selected0['Path'] = df['State'].apply(lambda x: x.get('Path') if isinstance(x, dict) else None)\n",
    "df_selected0['User Agent'] = df['State'].apply(lambda x: x.get('User-Agent') if isinstance(x, dict) else None)\n",
    "\n",
    "# Inspect the extracted data\n",
    "print(\"Extracted Data:\\n\", df_selected0.head())\n",
    "\n",
    "# Save the cleaned and selected data to a new CSV file\n",
    "output_csv_path = \"/Users/vatsal/Desktop/SimplyTag/selected_data.csv\"\n",
    "df_selected0.to_csv(output_csv_path, index=False)\n",
    "print(f\"Selected data has been saved to {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we need to procced with the new selected data. So we are loading the selected_data file which we have created and then print the new data-frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/vatsal/Desktop/SimplyTag/selected_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_selected \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/vatsal/Desktop/SimplyTag/selected_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, low_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/vatsal/Desktop/SimplyTag/selected_data.csv'"
     ]
    }
   ],
   "source": [
    "df_selected = pd.read_csv(\"/Users/vatsal/Desktop/SimplyTag/selected_data.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_selected' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_selected\n\u001b[0;32m      2\u001b[0m df_selected[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUser Agent\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mnunique()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_selected' is not defined"
     ]
    }
   ],
   "source": [
    "df_selected\n",
    "df_selected[['User Agent']].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_selected' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRows with non-null \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUser Agent\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m, df_selected[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUser Agent\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnotnull()\u001b[38;5;241m.\u001b[39msum())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_selected' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Rows with non-null 'User Agent':\", df_selected['User Agent'].notnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_selected' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRows with non-null \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrace-id\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m, df_selected[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrace-id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnotnull()\u001b[38;5;241m.\u001b[39msum())\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRows with non-null \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHTTP Status Code\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m, df_selected[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHTTP Status Code\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnotnull()\u001b[38;5;241m.\u001b[39msum())\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRows with non-null \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPath\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m, df_selected[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPath\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnotnull()\u001b[38;5;241m.\u001b[39msum())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_selected' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Rows with non-null 'Trace-id':\", df_selected['Trace-id'].notnull().sum())\n",
    "print(\"Rows with non-null 'HTTP Status Code':\", df_selected['HTTP Status Code'].notnull().sum())\n",
    "print(\"Rows with non-null 'Path':\", df_selected['Path'].notnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows meeting all conditions: 0\n"
     ]
    }
   ],
   "source": [
    "valid_rows = df_selected[\n",
    "    df_selected['User Agent'].notnull() &\n",
    "    df_selected['Trace-id'].notnull() &\n",
    "    df_selected['HTTP Status Code'].notnull() &\n",
    "    df_selected['Path'].notnull()\n",
    "]\n",
    "print(\"Rows meeting all conditions:\", len(valid_rows))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows reserved with non-null 'User Agent': 179148\n",
      "Rows after dropping rows with missing values in critical fields: 0\n",
      "Cleaned data preview:\n",
      " Empty DataFrame\n",
      "Columns: [Timestamp, Trace-id, HTTP Status Code, Path, User Agent]\n",
      "Index: []\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Trace-id</th>\n",
       "      <th>HTTP Status Code</th>\n",
       "      <th>Path</th>\n",
       "      <th>User Agent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Timestamp, Trace-id, HTTP Status Code, Path, User Agent]\n",
       "Index: []"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Reserve rows where 'User Agent' is not null\n",
    "df_with_user_agent = df_selected[df_selected['User Agent'].notnull()]\n",
    "print(f\"Rows reserved with non-null 'User Agent': {len(df_with_user_agent)}\")\n",
    "\n",
    "# Step 2: Drop rows with missing values in critical fields from the reserved rows\n",
    "df_cleaned = df_with_user_agent.dropna(subset=['Trace-id', 'HTTP Status Code', 'Path'])\n",
    "print(f\"Rows after dropping rows with missing values in critical fields: {len(df_cleaned)}\")\n",
    "\n",
    "# Inspect the cleaned data\n",
    "print(\"Cleaned data preview:\\n\", df_cleaned.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lines of code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values Summary:\n",
      " Timestamp                 0\n",
      "Trace-id              98843\n",
      "HTTP Status Code    1832377\n",
      "Path                1847425\n",
      "User Agent          1965256\n",
      "dtype: int64\n",
      "Data after handling missing values while retaining rows with 'User Agent':\n",
      " Empty DataFrame\n",
      "Columns: [Timestamp, Trace-id, HTTP Status Code, Path, User Agent]\n",
      "Index: []\n",
      "Number of unique User Agents: 0\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "missing_summary = df_selected.isnull().sum()\n",
    "print(\"Missing Values Summary:\\n\", missing_summary)\n",
    "\n",
    "# Retain rows where 'User Agent' is not null\n",
    "df_with_user_agent = df_selected[df_selected['User Agent'].notnull()]\n",
    "df_with_user_agent['User Agent'].nunique()\n",
    "# Drop rows with missing values in critical fields but retain rows with valid 'User Agent'\n",
    "df_final = df_with_user_agent.dropna(subset=['Trace-id', 'HTTP Status Code', 'Path']).copy()\n",
    "\n",
    "print(\"Data after handling missing values while retaining rows with 'User Agent':\\n\", df_final.head())\n",
    "print(\"Number of unique User Agents:\", df_final['User Agent'].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data after removing duplicates: 106480 rows\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Trace-id</th>\n",
       "      <th>HTTP Status Code</th>\n",
       "      <th>Path</th>\n",
       "      <th>User Agent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2024-11-29T23:00:22.7170405Z</td>\n",
       "      <td>3dff50eaf246e6cc55173db4092d5187</td>\n",
       "      <td>200.0</td>\n",
       "      <td>/api/v2/versions</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2024-11-29T23:00:36.5883159Z</td>\n",
       "      <td>bb52744079ab054a8189bf62d8602370</td>\n",
       "      <td>200.0</td>\n",
       "      <td>/api/v2/versions</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>2024-11-29T23:01:12.6574762Z</td>\n",
       "      <td>02316aed810d686f6c7a5fbe693f10ca</td>\n",
       "      <td>200.0</td>\n",
       "      <td>/api/v2/versions</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>2024-11-29T23:01:22.8895503Z</td>\n",
       "      <td>005331c774905da73d2d938d1a1fb153</td>\n",
       "      <td>200.0</td>\n",
       "      <td>/api/v2/versions</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>2024-11-29T23:01:36.7376115Z</td>\n",
       "      <td>83019acd478b0e66cf0e90a932f663de</td>\n",
       "      <td>200.0</td>\n",
       "      <td>/api/v2/versions</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2144327</th>\n",
       "      <td>2024-11-19T22:58:58.9054399Z</td>\n",
       "      <td>8f56736ae00d6d0905149e664675d3e3</td>\n",
       "      <td>200.0</td>\n",
       "      <td>/api/v2/versions</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2144340</th>\n",
       "      <td>2024-11-19T22:59:03.0837242Z</td>\n",
       "      <td>938fb7915bf6d0bbf1f0cd0f0155acf1</td>\n",
       "      <td>200.0</td>\n",
       "      <td>/api/v2/versions</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2144369</th>\n",
       "      <td>2024-11-19T22:59:55.7616546Z</td>\n",
       "      <td>0cfdcd4505ca4f1969017713cc115bff</td>\n",
       "      <td>200.0</td>\n",
       "      <td>/api/v2/versions</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2144390</th>\n",
       "      <td>2024-11-19T22:59:59.0769271Z</td>\n",
       "      <td>c04e0c475d8a6a9e673f00ae3bb74e41</td>\n",
       "      <td>200.0</td>\n",
       "      <td>/api/v2/versions</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2144403</th>\n",
       "      <td>2024-11-19T23:00:03.2865924Z</td>\n",
       "      <td>e633e492116f3febe619bfed47449e89</td>\n",
       "      <td>200.0</td>\n",
       "      <td>/api/v2/versions</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>106480 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Timestamp                          Trace-id  \\\n",
       "12       2024-11-29T23:00:22.7170405Z  3dff50eaf246e6cc55173db4092d5187   \n",
       "28       2024-11-29T23:00:36.5883159Z  bb52744079ab054a8189bf62d8602370   \n",
       "62       2024-11-29T23:01:12.6574762Z  02316aed810d686f6c7a5fbe693f10ca   \n",
       "75       2024-11-29T23:01:22.8895503Z  005331c774905da73d2d938d1a1fb153   \n",
       "88       2024-11-29T23:01:36.7376115Z  83019acd478b0e66cf0e90a932f663de   \n",
       "...                               ...                               ...   \n",
       "2144327  2024-11-19T22:58:58.9054399Z  8f56736ae00d6d0905149e664675d3e3   \n",
       "2144340  2024-11-19T22:59:03.0837242Z  938fb7915bf6d0bbf1f0cd0f0155acf1   \n",
       "2144369  2024-11-19T22:59:55.7616546Z  0cfdcd4505ca4f1969017713cc115bff   \n",
       "2144390  2024-11-19T22:59:59.0769271Z  c04e0c475d8a6a9e673f00ae3bb74e41   \n",
       "2144403  2024-11-19T23:00:03.2865924Z  e633e492116f3febe619bfed47449e89   \n",
       "\n",
       "         HTTP Status Code              Path User Agent  \n",
       "12                  200.0  /api/v2/versions        NaN  \n",
       "28                  200.0  /api/v2/versions        NaN  \n",
       "62                  200.0  /api/v2/versions        NaN  \n",
       "75                  200.0  /api/v2/versions        NaN  \n",
       "88                  200.0  /api/v2/versions        NaN  \n",
       "...                   ...               ...        ...  \n",
       "2144327             200.0  /api/v2/versions        NaN  \n",
       "2144340             200.0  /api/v2/versions        NaN  \n",
       "2144369             200.0  /api/v2/versions        NaN  \n",
       "2144390             200.0  /api/v2/versions        NaN  \n",
       "2144403             200.0  /api/v2/versions        NaN  \n",
       "\n",
       "[106480 rows x 5 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop duplicate entries based on critical columns\n",
    "df_dropduplicate = df_dropped.drop_duplicates(subset=['Timestamp', 'Trace-id', 'HTTP Status Code', 'Path'])\n",
    "\n",
    "print(f\"Data after removing duplicates: {len(df_dropduplicate)} rows\")\n",
    "df_dropduplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             Timestamp        Date  Hour Day_of_week\n",
      "12 2024-11-29 23:00:22.717040500+00:00  2024-11-29    23      Friday\n",
      "28 2024-11-29 23:00:36.588315900+00:00  2024-11-29    23      Friday\n",
      "62 2024-11-29 23:01:12.657476200+00:00  2024-11-29    23      Friday\n",
      "75 2024-11-29 23:01:22.889550300+00:00  2024-11-29    23      Friday\n",
      "88 2024-11-29 23:01:36.737611500+00:00  2024-11-29    23      Friday\n"
     ]
    }
   ],
   "source": [
    "# Convert timestamp to datetime format\n",
    "df_dropduplicate['Timestamp'] = pd.to_datetime(df_dropduplicate['Timestamp'], errors='coerce')\n",
    "\n",
    "# Remove rows with invalid timestamps\n",
    "df_dropduplicate = df_dropduplicate.dropna(subset=['Timestamp'])\n",
    "\n",
    "# Optionally, create new time-related columns\n",
    "df_dropduplicate['Date'] = df_dropduplicate['Timestamp'].dt.date\n",
    "df_dropduplicate['Hour'] = df_dropduplicate['Timestamp'].dt.hour\n",
    "df_dropduplicate['Day_of_week'] = df_dropduplicate['Timestamp'].dt.day_name()\n",
    "\n",
    "print(df_dropduplicate[['Timestamp', 'Date', 'Hour', 'Day_of_week']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample User Agent Normalization:\n",
      "    User Agent User_Agent_Normalized\n",
      "12        NaN                   NaN\n",
      "28        NaN                   NaN\n",
      "62        NaN                   NaN\n",
      "75        NaN                   NaN\n",
      "88        NaN                   NaN\n"
     ]
    }
   ],
   "source": [
    "# Normalize user agent strings (basic example)\n",
    "df_dropduplicate['User_Agent_Normalized'] = df_dropduplicate['User Agent'].str.lower().str.strip()\n",
    "\n",
    "print(\"Sample User Agent Normalization:\\n\", df_dropduplicate[['User Agent', 'User_Agent_Normalized']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data after filtering valid status codes: 102767 rows\n"
     ]
    }
   ],
   "source": [
    "# Define valid HTTP status codes\n",
    "valid_status_codes = [200, 404]\n",
    "\n",
    "# Filter rows with valid status codes\n",
    "df_dropduplicate = df_dropduplicate[df_dropduplicate['HTTP Status Code'].isin(valid_status_codes)]\n",
    "\n",
    "print(f\"Data after filtering valid status codes: {len(df_dropduplicate)} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   User Agent user_agent_category\n",
      "12        NaN             Unknown\n",
      "28        NaN             Unknown\n",
      "62        NaN             Unknown\n",
      "75        NaN             Unknown\n",
      "88        NaN             Unknown\n"
     ]
    }
   ],
   "source": [
    "def categorize_user_agent(user_agent):\n",
    "    if pd.isnull(user_agent) or user_agent.strip() == \"\":\n",
    "        return \"Unknown\"\n",
    "    elif \"bot\" in user_agent.lower() or \"spider\" in user_agent.lower():\n",
    "        return \"Bot\"\n",
    "    elif \"curl\" in user_agent.lower() or \"wget\" in user_agent.lower():\n",
    "        return \"Script\"\n",
    "    elif \"mozilla\" in user_agent.lower():\n",
    "        return \"Browser\"\n",
    "    else:\n",
    "        return \"Other\"\n",
    "\n",
    "# Apply the categorization using .loc\n",
    "df_dropduplicate.loc[:, 'user_agent_category'] = df_dropduplicate['User Agent'].apply(categorize_user_agent)\n",
    "\n",
    "print(df_dropduplicate[['User Agent', 'user_agent_category']].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "User Agent    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dropduplicate[['User Agent']].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
